{
  
  "0": {
    "title": "Autonomous",
    "content": "Autonomous mode . . Writing policies | Metrics collection | Web User Interface | . Autonomous reads the scenarios to execute from the policy file, and runs them: . The matches are combined together and deduplicated to produce an initial working set | They are run through a series of filters | For all the items remaining after the filters, all actions are executed | $ seal autonomous --help . Writing policies . A minimal policy file, doing nothing, looks like this: . config: loopsNumber: 1 # will execute the provided scenarios once and then exit nodeScenarios: [] podScenarios: [] . A more interesting schema, that kills a random pod in default namespace every 1-30 seconds: . config: # we don&#39;t set loopsNumber, so it will loop indefinitely minSecondsBetweenRuns: 1 maxSecondsBetweenRuns: 30 nodeScenarios: [] podScenarios: - name: &quot;delete random pods in default namespace&quot; match: - namespace: name: &quot;default&quot; filters: - randomSample: size: 1 actions: - kill: probability: 0.77 force: true . A full featured example listing most of the available options can be found in the tests. . The schemas are validated against the powerful JSON schema. . Metrics collection . Autonomous mode also comes with the ability for metrics useful for monitoring to be collected. PowerfulSeal currently has a stdout, Prometheus and Datadog collector. However, metric collectors are easily extensible so it is easy to add your own. More details can be found here. . Web User Interface . ⚠️ If you’re not going to use the UI, use the flag --headless to disable it . PowerfulSeal comes with a web interface to help you navigate Autonomous Mode. Features include: . viewing logs | viewing the policy | . .",
    "url": "http://localhost:4000/autonomous-mode",
    "relUrl": "/autonomous-mode"
  }
  ,"1": {
    "title": "Cloud Provider",
    "content": "Cloud Provider Requirements . . SSH | Azure | AWS | OpenStack | GCP | . SSH . In all cases, the SSH Keys must be set up for SSH Client access of the nodes. . Note: With GCP, running gcloud compute config-ssh makes SSHing to node instances easier by adding an alias for each instance to the user SSH configuration (~/.ssh/config) file and then being able to use the generated file with --ssh-path-to-private-key argument. . Azure . The credentials to connect to Azure may be specified in one of two ways: . Supply the full path to an Azure credentials file in the environment variable AZURE_AUTH_LOCATION. This is the easiest method. The credentials file can be generated via az aks get-credentials -n &lt;cluster name&gt; -g &lt;resource group&gt; -a -f &lt;destination credentials file&gt; | Supply the individual credentials in the environment variables: AZURE_SUBSCRIPTION_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET, AZURE_TENANT_ID | AWS . The credentials to connect to AWS are specified the same as for the AWS CLI . OpenStack . The easiest way to use PowerfulSeal, is to download and source the OpenRC file you can get from Horizon. It should ask you for your password, and it should set all the OS_* variables for you. Alternatively, you can set them yourself. . Both approaches are detailed in the official documentation. . GCP . Google Cloud SDK and kubectl are required . The GCP cloud driver supports managed (GKE) and custom Kubernetes clusters running on top of Google Cloud Compute. . For setting up PowerfulSeal, the first step is configuring gcloud SDK (as PowerfulSeal will work with your configured project and region) and pointing kubectl to your cluster. Both can be configured easily following this tutorial (For GKE!). In case you don’t want to use the default project/region of gcloud SDK, you can point PowerfulSeal to the correct one (in json) with --gcp-config-file argument. . For being able to run node related commands, credentials have to be specified in one of these ways: . Service account (Recommended): a Google account that is associated with your GCP project, as opposed to a specific user. PowerfulSeal uses the environment variable and is pretty straightforward to set up using this tutorial. | User account: Not recommended as you can reach easily reach a “quota exceeded” or “API not enabled” error. PowerfulSeal uses auto-discovery and to get it working just follow this. | Having configuration ready and ssh connection to the node instances working, you can start playing with PowerfulSeal with this example: powerfulseal interactive --kubeconfig ~/.kube/config --gcp --inventory-kubernetes --ssh-allow-missing-host-keys --ssh-path-to-private-key ~/.ssh/google_compute_engine --remote-user myuser . Note: In case of running inside Pyenv and getting python2 command not found error when running gcloud (and you want to run PowerfulSeal with Python 3+), this might be useful, as gcloud requires Python2. .",
    "url": "http://localhost:4000/cloud-provider-requirements",
    "relUrl": "/cloud-provider-requirements"
  }
  ,"2": {
    "title": "Contribute",
    "content": "Contribute to PowerfulSeal . . Overview | Contribution Licensing | Testing Installation | Usage | | . Overview . If you’d like to help us improve and extend this project, then we welcome your contributions! . Below you will find some basic steps required to be able to contribute to the project. If you have any questions about this process or any other aspect of contributing to a Bloomberg open source project, feel free to send an email to opensource@bloomberg.net and we’ll get your questions answered as quickly as we can. . Pull Requests are always welcome, however they will only be accepted if they provide a reasonably test coverage. . Contribution Licensing . Since PowerfulSeal is distributed under the terms of the Apache Version 2 license, contributions that you make are licensed under the same terms. In order for us to be able to accept your contributions, we will need explicit confirmation from you that you are able and willing to provide them under these terms, and the mechanism we use to do this is called a Developer’s Certificate of Origin DCO. This is very similar to the process used by the Linux(R) kernel, Samba, and many other major open source projects. . To participate under these terms, all that you must do is include a line like the following as the last line of the commit message for each commit in your contribution: . Signed-Off-By: Random J. Developer &lt;random@developer.example.org&gt; The simplest way to accomplish this is to add `-s` or `--signoff` to your `git commit` command. You must use your real name (sorry, no pseudonyms, and no anonymous contributions). . Testing . PowerfulSeal uses tox to test multiple Python versions in a straightforward manner. . Installation . In order to use tox, tox must be installed and Python binaries for the versions listed in tox.ini (3.6, 3.7 as of writing) must be visible in your PATH. . Due to the difficulty in maintaining the required libraries for so many Python versions, it is recommended to use pyenv to install and manage multiple versions of Python. . The recommended installation steps are: . Install pyenv using the Basic GitHub Checkout method | Run pyenv install --list | For every version specified in tox.ini, find the latest patch version corresponding to the version (e.g., 3.6 -&gt; 3.6.5) and run pyenv install [version] | In this project’s root directory, run pyenv local [versions], where [versions] is a space-separated list of every version you just installed (e.g., pyenv local 3.6.5 3.7.0) | Run pyenv which 3.6, pyenv which 3.7, etc. and ensure there are no errors and the output path is a .pyenv directory | Usage . With the installation complete, simply run tox (or the analagous make test). If you are running on a machine with inotifywait installed (i.e., a UNIX machine), you can run make watch and run tests automatically when you run changes. . You can also run tests for specific versions by running tox -e [version(s)] (e.g., tox -e py36). Additionally, if you need to reinstall dependencies, you can use the -r flag. .",
    "url": "http://localhost:4000/contribute",
    "relUrl": "/contribute"
  }
  ,"3": {
    "title": "Getting Started",
    "content": "Getting started . . Docker | . PowerfulSeal is available to install through pip: . pip install powerfulseal powerfulseal --help # or seal --help . To start the web interface, use flags --server --server-host [HOST] --server-port [PORT] when starting PowerfulSeal in autonomous mode and visit the web server at http://HOST:PORT/. . Python 3.6, Python 3.7 and Python 3.8 are supported. . Docker . The automatically built docker images are now available on docker hub . docker pull bloomberg/powerfulseal:2.7.0 .",
    "url": "http://localhost:4000/getting-started",
    "relUrl": "/getting-started"
  }
  ,"4": {
    "title": "In-depth topics",
    "content": "In-depth topics . . Metric collection Description | Usage StdoutCollector (default) | PrometheusCollector | DatadogCollector | | Use Case: Prometheus + Grafana + AlertManager | Use Case: Datadog | | Inventory file | Extend Powerfulseal Custom Metric Collectors | Custom Cloud Drivers | Custom Filters | | . Metric collection . Description . The purpose of metric collection is to keep track of events which you may consider useful to be monitored (e.g., via Grafana). Three metric collectors have been implemented: . StdoutCollector, a “no-op” collector which simply prints that an event has occurred to stdout | PrometheusCollector, which sends metrics to the default Prometheus registry and exposes them via Prometheus internal web server | DatadogCollector, which sends metrics to the DogStatsD aggregation service bundled with the Datadog Agent. | . Like cloud drivers, metric collectors are extensible by subscribing to the AbstractCollector interface. Likewise, AbstractCollector makes it easy to add your own metrics. . The metric collectors collect the following events: . Metric Labels (Metadata) Description Justification . seal_pod_kills_total | status, namespace, name | Number of pods killed (including failures) | Example alerts include number of pod kills failing in the past five minutes or the ratio between successful and failed pod kills in a time range. If a pod cannot be killed, it could be an an unresponsive state or there may be a problem with keeping Kubernetes synchronised to the node. | . seal_nodes_stopped_total | status, uid, name | Number of nodes killed (including failures) | Example alerts include number of node stops failing in the past five minutes or the ratio between successful and failed node stops. If a node cannot be stopped, it could be in an unresponsive state. | . seal_execute_failed_total | uid, name | Tracks failure of command executions | Commands executions failing is a general case of errors which should be brought to the user’s attention. | . seal_empty_filter_total | N/A | Cases where filtering returns an empty result | If the user’s policy is designed to model common levels of failures, then having no nodes/pods after filtering could mean that insufficient resources have been provisioned for the system to withstand failure. | . seal_probability_filter_not_passed_total | N/A | Cases where the probability filter decides to skip all nodes | Useful to track long-term in order to ensure that probability distribution is as expected. | . seal_empty_match_total | source (either nodes or pods) | Cases where matching returns an empty result | See seal_empty_filter_total | . Usage . StdoutCollector (default) . To print metrics to stdout, use the --stdout-collector flag. . PrometheusCollector . To collect metrics, run PowerfulSeal with the --prometheus-collector flag, along with the --prometheus-host and --prometheus-port flags. Metrics will be exposed under a web server with URL http://HOST:PORT/metrics. . DatadogCollector . This collector relies on DogStatsD server (bundled with the Datadog Agent), so a working instance is expected. . To collect metrics, run PowerfulSeal with the --datadog-collector flag. . Use Case: Prometheus + Grafana + AlertManager . . A common use case is to use a combination of Prometheus, Grafana and AlertManager in order to increase visibility of potential issues. . In order to configure this integration, follow the steps below. (The below instructions assume that Prometheus and Grafana are already set up.) . Open your Prometheus configuration file (e.g., /etc/prometheus/prometheus.yml) and add a scrape_configs job with the host IP and a chosen port for the server PowerfulSeal will be run on: yaml scrape_configs: job_name: powerfulseal scrape_interval: 5s scrape_timeout: 5s metrics_path: /metrics scheme: http static_configs: targets: labels: name: powerfulseal . | . | . | . | Add an alert file (e.g., seal.alerts.yml) to your Prometheus configuration path with your alerting rules (example here) | Update alertmanager.yml to handle the alerting rules, for example: global: resolve_timeout: 5m route: receiver: &#39;default&#39; routes: - receiver: &#39;seal&#39; match: type: seal # A list of notification receivers. receivers: - name: default email_configs: - name: seal email_configs: . | Start PowerfulSeal with the --prometheus-collector, --prometheus-host and --prometheus-port flags, and restart Prometheus. Metrics should begin to appear. | Ensure Grafana has your Prometheus server added as a data source and create a new Grafana dashboard with the metrics (example here - note that the data source name may need to be changed) | Use Case: Datadog . . It’s common to use Datadog in order to increase visibility of potential issues. . In order to configure this properly, follow the steps below. (The below instructions assume that Datadog Agent is already set up.) . Start PowerfulSeal with the --datadog-collector flag. Metrics should begin to appear on Datadog. . | Create a new dashboard with the collected metrics (example here). . | Configure alerting with Monitors, to give you the ability to know when critical changes are occurring. . | Inventory file . PowerfulSeal can use an ansible-style inventory file (in ini format) . [mygroup] myhost01 [mygroup2] myhost02 [some_hosts] myhost01 myhost02 . Extend Powerfulseal . Custom Metric Collectors . Custom Cloud Drivers . Custom Filters .",
    "url": "http://localhost:4000/in-depth-topics",
    "relUrl": "/in-depth-topics"
  }
  ,"5": {
    "title": "Modes",
    "content": "Modes . PowerfulSeal works in several modes: . Interactive mode is designed to allow you to discover your cluster’s components and manually break things to see what happens. It operates on nodes, pods, deployments and namespaces. . | Autonomous mode reads a policy file, which can contain any number of pod and node scenarios. Each scenario describes a list of matches, filters and actions to execute on your cluster, and will be executed in a loop. . | Label mode allows you to specify which pods to kill with a small number of options by adding seal/ labels to pods. This is a more imperative alternative to autonomous mode. . | .",
    "url": "http://localhost:4000/modes",
    "relUrl": "/modes"
  }
  ,"6": {
    "title": "Welcome",
    "content": "A powerful testing tool for Kubernetes clusters. . PowerfulSeal adds chaos to your Kubernetes clusters, so that you can detect problems in your systems as early as possible. It kills targeted pods and takes VMs up and down. . Get started now View it on GitHub . . Highlights . works with OpenStack, AWS, Azure, GCP and local machines | speaks Kubernetes natively | interactive and autonomous, policy-driven mode | web interface to interact with PowerfulSeal | metric collection and exposition to Prometheus or Datadog | minimal setup, easy yaml-based policies | easy to extend | . License . Powerfulseal is distributed by an Apache-2.0. .",
    "url": "http://localhost:4000/",
    "relUrl": "/"
  }
  ,"7": {
    "title": "Interactive",
    "content": "Interactive mode . $ seal interactive --help . Make sure you hit tab for autocompletion - that’s what really makes the seal easy to use. . Here’s a sneak peek of what you can do in the interactive mode: . . .",
    "url": "http://localhost:4000/interactive-mode",
    "relUrl": "/interactive-mode"
  }
  ,"8": {
    "title": "Label",
    "content": "Label Mode . Label mode is a more imperative alternative to autonomous mode, allowing you to specify which specific per-pod whether a pod should be killed, the days/times it can be killed and the probability of it being killed. . Label mode is a good way to have more granular control over which pods are killed at the expense of greater verbosity. This trade-off is especially useful if you have a large number of pods (in the hundreds or even greater magnitudes) but only want to run PowerfulSeal on a few pods, or if your Kubernetes cluster has pods where you want full confidence that PowerfulSeal would not kill. . Usage . Labels can be manually set using the kubectl label pods [POD NAME] [LABEL] command. Once labels are set, label mode can be run by using the --label flag. You may also wish to set the --min-seconds-between-runs and --max-seconds-between-runs flags which default to 0 and 300 respectively. . To reduce the processing time needed to filter a large number of pods, you can instruct PowerfulSeal to only look up pods under a specific namespace by using the --kubernetes-namespace argument. This behaves similar to kubectl, where not specifying the argument defaults PowerfulSeal to the default namespace, whereas specifying an empty value (--kubernetes-namespace=) retrieves all pods across all namespaces. . Annotations . The values labels can have in Kubernetes is restricted, so some of the configuration below is not possible with labels (seal/days at the moment). To work around this, you can use any of the labels specified here as annotations instead. If you specify a key both as a label and an annotation, the label value will be preferred. . Example . Suppose we have pods my-app-1, my-app-2, etc. under the default namespace in a system which is designed to handle the failure of one my-app pod. In this case, we can make the decision to label the first application pod: . To allow PowerfulSeal to act on the pod, run kubectl label pods my-app-1 seal/enabled=true | To next increase the randomness of when the pod can fail (reflecting the unexpectedness of real world failures), run kubectl label pods my-app-1 seal/kill-probability=0.5 | Finally, to ensure we’re present in the office if the resilience of the system fails, we can set labels so that our pod is only killed during working hours with seal/days=&quot;mon,tue,way,thu,fri&quot;, seal/start-time=10-00-00 and seal/end-time=17-30-00 | All of these labels are optional. For additional labels and their defaults, see the reference below. . To finally get PowerfulSeal running, assuming our kube-config is at ~/.kube/config and we’e using the no-cloud driver, run: powerfulseal label --inventory--kubernetes --kube-config ~/.kube/config. . If we were to change the namespace of the my-app-* pods to, for example, production, PowerfulSeal can be either run with the --kubernetes-namespace=production argument to get all pods with the production namespace, or --kubernetes-namespace= to get all pods across all namespaces (not recommended due to poor performance when is a large number of pods). . Reference . Label Description Default . seal/enabled | Either “true” or “false” | “false” | . seal/force-kill | Either “true” or “false” | “false” | . seal/kill-probability | A value between “0” and “1” inclusive describing the probability that a pod should be killed | “1” | . seal/days | A comma-separated string consisting of “mon”, “tue”, “wed”, “thu”, “fri”, “sat”, “sun”, describing the days which the pod can be killed | “mon,tue,wed,thu,fri” | . seal/start-time | A value “HH-MM-SS” describing the inclusive start boundary of when a pod can be killed in the local timezone | “10-00-00” | . seal/end-time | A value “HH-MM-SS” describing the exclusive end boundary of when a pod can be killed in the local time zone | “17-30-00” | .",
    "url": "http://localhost:4000/label-mode",
    "relUrl": "/label-mode"
  }
  ,"9": {
    "title": "Setup",
    "content": "Setup Powerfulseal . . Running inside of the cluster | Running outside of the cluster | Minikube setup | . The setup depends on whether you run PowerfulSeal inside or outside of your cluster. . Running inside of the cluster . If you’re running inside of the cluster (for example from the docker image), the setup is pretty easy. . You can see an example of how to do it in ./kubernetes. The setup involves: . creating RBAC rules to allow the seal to list, get and delete pods, | creating a powerfulseal configmap and deployment your scenarios will live in the configmap | if you’d like to use the UI, you’ll probably also need a service and ingress | make sure to use --use-pod-delete-instead-of-ssh-kill flag to not need to configure SSH access for killing pods | . | profit! the Seal will self-discover the way to connect to kubernetes and start executing your policy | . | . Running outside of the cluster . If you’re running outside of your cluster, the setup will involve: . pointing PowerfulSeal at your Kubernetes cluster by giving it a Kubernetes config file | pointing PowerfulSeal at your cloud by specifying the cloud driver to use and providing credentials | making sure the seal can SSH into the nodes in order to execute docker kill command | writing a set of policies | . It should look something like this. . Minikube setup . It is possible to test a subset of Seal’s functionality using a minikube setup. . To achieve that, please inspect the Makefile. You will need to override the ssh host, specify the correct username and use minikube’s ssh keys. . If you’d like to test out the interactive mode, start with this: . seal -vv interactive --no-cloud --inventory-kubernetes --ssh-allow-missing-host-keys --remote-user docker --ssh-path-to-private-key `minikube ssh-key` --ssh-password `minikube ssh-password` --override-ssh-host `minikube ip` . For label mode, try something like this: . seal -vv label --no-cloud --min-seconds-between-runs 3 --max-seconds-between-runs 10 --inventory-kubernetes --ssh-allow-missing-host-keys --remote-user docker --ssh-path-to-private-key `minikube ssh-key` --ssh-password `minikube ssh-password` --override-ssh-host `minikube ip` . For autonomous mode, this should get you started: . seal -vv autonomous --no-cloud --policy-file ./examples/policy_kill_random_default.yml --inventory-kubernetes --prometheus-collector --prometheus-host 0.0.0.0 --prometheus-port 9999 --ssh-allow-missing-host-keys --remote-user docker --ssh-path-to-private-key `minikube ssh-key` --ssh-password `minikube ssh-password` --override-ssh-host `minikube ip` --host 0.0.0.0 --port 30100 .",
    "url": "http://localhost:4000/setup",
    "relUrl": "/setup"
  }
  ,"10": {
    "title": "Terminology",
    "content": "Terminology .",
    "url": "http://localhost:4000/terminology",
    "relUrl": "/terminology"
  }
  
}